{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugo Da Costa, Alban Guerbois\n",
    "M1 I2D - Projet MachineLearning/DataScience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree et  Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soit $X$ la matrice représentant l'ensemble des données que nous avons. \n",
    "On note : $ X \\in \\mathbb{R^{n*d}} $\n",
    "\n",
    "Chacun de ce des $d-uplets$, soit chaque vecteur de ${R^{d}}$, décrit 1 individu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Définition des classes et méthodes de notre structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self, father=None):\n",
    "        self.father = father #type = Node\n",
    "        self.question = None #type = Question\n",
    "        self.leftSon = None  #type = Node\n",
    "        self.rightSon = None  #type = Node\n",
    "        self.predictedClass = None #type = int ou None (selon si feuille ou pas)\n",
    "    \n",
    "    \n",
    "    def get_depth(self):\n",
    "        if self.father == None:\n",
    "            return 0\n",
    "        return (self.father.get_depth() + 1)\n",
    "\n",
    "    \n",
    "# Mutateurs\n",
    "    def setLeftSon(self, node):\n",
    "        self.leftSon = node\n",
    "        \n",
    "    def setRightSon(self, node):\n",
    "        self.rightSon = node\n",
    "        \n",
    "    def setPrediction(self, label):\n",
    "        self.predictedClass = label\n",
    "    \n",
    "    def setQuestion(self, q):\n",
    "        self.question = q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans toute la suite, on notera subsetOfX et  subsetOfy pour évoquer un certain sous-ensemble des données incarnant le training_set, ou même un sous ensemble de ce training set.\n",
    "\n",
    "En effet, chaque arbre de décision recevra en entrée un training_set issu des data globales, afin de créer ses noeuds. Il recevra ensuite le test_set afin de déduire leur label, et on vérifiera la qualité de ces prédictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getGiniImpurity(subsetOfy):\n",
    "    labels, occurences = np.unique(subsetOfy, return_counts=True)\n",
    "    return 1 - sum( (occurences/np.size(subsetOfy) )**2 )\n",
    "\n",
    "\n",
    "def getInformationGain(subsetOfy, subOfsub1, subOfsub2):\n",
    "    gini_init = getGiniImpurity(subsetOfy)\n",
    "    gini_subset1 = getGiniImpurity(subOfsub1)\n",
    "    gini_subset2 = getGiniImpurity(subOfsub2)\n",
    "    \n",
    "    return gini_init - ( gini_subset1*np.size(subOfsub1)/np.size(subsetOfy) + gini_subset2*np.size(subOfsub2)/np.size(subsetOfy) )\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Question(object):\n",
    "    def __init__(self, testedValue, testedDimension):\n",
    "        self.testedValue = testedValue\n",
    "        self.testedDimension = testedDimension\n",
    "        \n",
    "    def askQuestion(self, nuplet):\n",
    "        if nuplet[self.testedDimension] > self.testedValue :\n",
    "            return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Classe, cas général"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On considère que notre set de données se récupère souvent sous la forme d'un DataFrame utilisable via la librairie Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecisionTree(object):\n",
    "    def __init__(self, root=Node(), max_depth=None):\n",
    "        self.root = root\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    # Dans notre D.T, un noeud est une feuille si sa Gini=0\n",
    "    def isLeaf(self, subsetOfy):\n",
    "        if getGiniImpurity(subsetOfy) == 0 :\n",
    "            return True\n",
    "        else :\n",
    "            return False\n",
    "    \n",
    "    # ou bien s'il a atteint à la limite de la profondeur_maximale\n",
    "    def isBeforeMaxDepth(self, node):\n",
    "        if self.max_depth == None :\n",
    "            return True\n",
    "        \n",
    "        elif self.max_depth > node.get_depth() :\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Les fonctions split et best_question sont cruciales : \n",
    "    \n",
    "    # A chaque noeud, on va trouver la question qui apporte le plus d'information sur le jeu de données\n",
    "    # et on va alors créer 2 fils a ce noeud, accompagnant les 2 sous matrices obtenus en divisant le subset via la question.\n",
    "    def split(self, current_node, subsetOfX, subsetOfy):\n",
    "        n, d = subsetOfX.shape\n",
    "        if (not self.isLeaf(subsetOfy)) and (n>3) and (self.isBeforeMaxDepth(current_node)) :\n",
    "            X_1, y_1, X_2, y_2 = self.best_question(current_node, subsetOfX, subsetOfy)\n",
    "            \n",
    "            # Cas dans lequel la collection contient des éléments identiques sur leurs features,\n",
    "            # mais dont les labelssont différents.\n",
    "            if X_1.size == 0 or X_2.size == 0:\n",
    "                current_node.predictedClass = np.bincount(subsetOfy).argmax()\n",
    "            \n",
    "            else :\n",
    "                self.split(current_node.leftSon, X_1, y_1)\n",
    "                self.split(current_node.rightSon, X_2, y_2)\n",
    "        \n",
    "        else :\n",
    "            current_node.setPrediction( np.argmax(np.bincount(subsetOfy)) )\n",
    "        \n",
    "    \n",
    "    \n",
    "    # Cette \"meilleure question\" elle, s'obtient en testant pleins de questions (sur différentes valeurs et différentes dimensions)\n",
    "    def best_question(self, node, subsetOfX, subsetOfy):\n",
    "        n , d = subsetOfX.shape  # n = nb of individuals in the subMatrix of X / d = number of features in...\n",
    "        \n",
    "        best_d_test, best_value_test, best_filter, max_gain_info = 0, 0, [], -3\n",
    "        \n",
    "        for _ in range(1000):\n",
    "            \n",
    "            d_test = random.choice([dim for dim in subsetOfX.columns])\n",
    "            \n",
    "            low_bound = min( subsetOfX[d_test] )\n",
    "            up_bound = max( subsetOfX[d_test] )\n",
    "            value_test = random.uniform(low_bound, up_bound)\n",
    "            \n",
    "            filter1 = subsetOfX[d_test] > value_test\n",
    "            subsubOfX_1 = subsetOfX[filter1]\n",
    "            subsubOfy_1 = subsetOfy[filter1]\n",
    "            \n",
    "            filter2 = np.logical_not(filter1)\n",
    "            subsubOfX_2 = subsetOfX[filter2]\n",
    "            subsubOfy_2 = subsetOfy[filter2]\n",
    "            \n",
    "            gain_info = getInformationGain(subsetOfy, subsetOfy[filter1], subsetOfy[filter2])\n",
    "            \n",
    "            if max_gain_info < gain_info :\n",
    "                best_d_test = d_test\n",
    "                best_value_test = value_test\n",
    "                best_filter = filter1\n",
    "                max_gain_info = gain_info\n",
    "        \n",
    "        node.setLeftSon( Node(father=node) )\n",
    "        node.setRightSon( Node(father=node) )\n",
    "        node.setQuestion( Question(best_value_test, best_d_test) )\n",
    "        \n",
    "        return subsetOfX[best_filter], subsetOfy[best_filter], subsetOfX[np.logical_not(filter1)], subsetOfy[np.logical_not(filter1)]\n",
    "                \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Tout ce travail sur le Training set et les subsetofTrainingSet était pour obtenir des questions cohérentes\n",
    "    # nous n'avons plus qu'a les appliquer sur des vecteurs de R^n\n",
    "    \n",
    "    def get_tuple_prediction(self, row):\n",
    "        current = self.root\n",
    "        while ( (current.leftSon != None)  and (current.rightSon != None) ):\n",
    "            if row[current.question.testedDimension] > current.question.testedValue :\n",
    "                current = current.leftSon\n",
    "            else :\n",
    "                current = current.rightSon\n",
    "        return current.predictedClass        \n",
    "                \n",
    "    \n",
    "    \n",
    "    def test_data(self, X_train, y_train, X_test, y_test):\n",
    "        # Création des noeuds de l'arbre via le trainig set\n",
    "        noeud = self.root\n",
    "        self.split(noeud, X_train, y_train)  #entrainement de notre arbre\n",
    "        \n",
    "        # Assignation de tous les classes aux vecteurs tests\n",
    "        predictions = pd.DataFrame(np.zeros(X_test.shape[0]), index=X_test.index)\n",
    "        \n",
    "        for index_row in X_test.index :\n",
    "            predictions.loc[index_row] = self.get_tuple_prediction(X_test.loc[index_row])\n",
    "\n",
    "        return predictions, confusion_matrix(y_test, predictions)\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RandomForest(object):\n",
    "    def __init__(self, nb_trees=10, prop_individuals_byTree=0.25, prop_dimensions_byTree=0.25, max_depth_by_tree=None):\n",
    "        self.nb_trees=nb_trees\n",
    "        self.prop1 = prop_individuals_byTree\n",
    "        self.prop2 = prop_dimensions_byTree\n",
    "        self.max_depth = max_depth_by_tree\n",
    "    \n",
    "    \n",
    "    \n",
    "# Chaque Decision Tree de notre Random Forest va recevoir une sous matrice de X_train, sur laquelle il pourra apprendre\n",
    "# Comme ca, chacun de nos arbres travaillera sur certaines parties du training_set, avec une certaine redondance.\n",
    "\n",
    "    def get_subpart_of_trainingSet(self, X_train, y_train):\n",
    "        subX, subY = [], []\n",
    "        \n",
    "        number_rows = int(self.prop1 * X_train.shape[0])\n",
    "        rows_set = set([])\n",
    "        \n",
    "        number_dimensions = int(self.prop2 * X_train.shape[1])\n",
    "        dimensions_set = set([])\n",
    "        \n",
    "        while len(rows_set) <= number_rows :\n",
    "            rand_row = random.choice( [i for i in X_train.index] )\n",
    "            if not (rand_row in rows_set) :\n",
    "                rows_set.add(rand_row)\n",
    "                \n",
    "        while len(dimensions_set) <= number_dimensions :\n",
    "            rand_dim = random.choice([d for d in X_train.columns])\n",
    "            if not (rand_dim in dimensions_set) :\n",
    "                dimensions_set.add(rand_dim)\n",
    "        \n",
    "        #print(\"individus testés : \", rows_set)\n",
    "        #print(\"dimensions testées : \", dimensions_set)\n",
    "        \n",
    "        # subX = X_train.iloc[[rows_set, dimensions_set]]\n",
    "        \n",
    "        for index_row in rows_set :\n",
    "            x0 = []\n",
    "            for index_dim in dimensions_set :\n",
    "                x0.append(X_train.at[index_row, index_dim])\n",
    "                \n",
    "            subX.append(x0)\n",
    "            subY.append(y_train.loc[index_row])\n",
    "            \n",
    "        subX = pd.DataFrame(np.array(subX), index=[i for i in rows_set], columns=[j for j in dimensions_set])\n",
    "        subY = pd.Series(np.array(subY), index=[i for i in rows_set])\n",
    "        \n",
    "        return subX, subY\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# Redefinition de la prédiction d'un n-uplet = Par la majorité de comment le classe tous les arbres (Vote majoritaire)\n",
    " \n",
    "    def test_data_rf(self, X_train, y_train, X_test, y_test):\n",
    "        \n",
    "        predictions_matrix = pd.DataFrame( np.zeros( (self.nb_trees, X_test.shape[0]) ), columns=X_test.index)\n",
    "        \n",
    "        compt_trees = 0\n",
    "        for tree_index in range(self.nb_trees):\n",
    "            d_tree = DecisionTree(max_depth = self.max_depth)\n",
    "            subX, subY = self.get_subpart_of_trainingSet(X_train, y_train)\n",
    "            pred_tree, Mat = d_tree.test_data(subX, subY, X_test, y_test)\n",
    "            \n",
    "            compt_trees += 1\n",
    "            #print(\"Confusion Matrix of this tree : \\n\", Mat)\n",
    "            print(compt_trees, \"tree(s) done.\")\n",
    "            #print( \"----------------\\n\\n\")\n",
    "            predictions_matrix.loc[tree_index] = pred_tree[0]\n",
    "        \n",
    "        predictions = pd.Series(np.zeros(X_test.shape[0]), index=X_test.index)\n",
    "        predictions_matrix.loc[:].astype(int)\n",
    "        \n",
    "        for data_index in X_test.index  :\n",
    "            # predictions_matrix = np.array( [ row.astype(int) for row in predictions_matrix.loc[:] ] )\n",
    "            predictions.loc[data_index] = np.argmax( np.bincount(predictions_matrix[data_index].astype(int)) )\n",
    "        \n",
    "        print( \"----------------\\n\\n\")\n",
    "        return predictions, confusion_matrix(y_test, predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
